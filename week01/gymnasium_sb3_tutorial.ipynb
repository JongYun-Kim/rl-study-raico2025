{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning with Gymnasium & Stable-Baselines3\n",
    "\n",
    "## \ud83c\udfaf Learning Objectives\n",
    "\n",
    "In this notebook, you'll learn:\n",
    "1. **Gymnasium API fundamentals** - How to create, interact with, and understand RL environments\n",
    "2. **Environment visualization** - Render and visualize agent behavior\n",
    "3. **Complete training pipeline** - Train a PPO agent using Stable-Baselines3\n",
    "4. **Evaluation & comparison** - Compare random vs. trained agents\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcda What is Reinforcement Learning?\n",
    "\n",
    "**Reinforcement Learning (RL)** is a type of machine learning where an **agent** learns to make decisions by interacting with an **environment**:\n",
    "\n",
    "```\n",
    "     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "     \u2502  Agent  \u2502\n",
    "     \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n",
    "          \u2502 action\n",
    "          \u2193\n",
    "   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "   \u2502 Environment \u2502\n",
    "   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "          \u2502 observation, reward\n",
    "          \u2193\n",
    "```\n",
    "\n",
    "- **Agent**: The learner/decision maker\n",
    "- **Environment**: The world the agent interacts with\n",
    "- **Action**: What the agent does\n",
    "- **Observation**: What the agent sees\n",
    "- **Reward**: Feedback signal (positive or negative)\n",
    "\n",
    "The agent's goal is to learn a **policy** (a strategy) that maximizes cumulative reward over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd27 Setup & Installation\n",
    "\n",
    "First, let's install the required packages. We'll need:\n",
    "- **gymnasium**: The environment API (successor to OpenAI Gym)\n",
    "- **stable-baselines3**: RL algorithms implementation (PyTorch-based)\n",
    "- **imageio**: For creating GIFs of episodes\n",
    "- **matplotlib**: For plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install gymnasium[classic-control,box2d] stable-baselines3 imageio imageio-ffmpeg matplotlib numpy --quiet\n",
    "\n",
    "print(\"\u2705 Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "import imageio\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(f\"Gymnasium version: {gym.__version__}\")\n",
    "print(\"\u2705 All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1\ufe0f\u20e3 Gymnasium API Fundamentals\n",
    "\n",
    "## Creating an Environment with `gym.make()`\n",
    "\n",
    "The first step in any RL project is creating an environment. Gymnasium provides a unified interface for many environments.\n",
    "\n",
    "### Key Parameters:\n",
    "- **`env_id`**: The name of the environment (e.g., \"CartPole-v1\")\n",
    "- **`render_mode`**: How to visualize the environment\n",
    "  - `None`: No rendering (fastest, for training)\n",
    "  - `\"human\"`: Real-time visualization window\n",
    "  - `\"rgb_array\"`: Returns images (for creating videos/GIFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our first environment - CartPole\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "print(f\"Environment: {env.spec.id}\")\n",
    "print(f\"Render mode: {env.render_mode}\")\n",
    "print(\"\\n\u2705 Environment created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Environment Spaces\n",
    "\n",
    "Every environment has two important spaces:\n",
    "\n",
    "### 1. **Observation Space** (`env.observation_space`)\n",
    "- Defines what the agent can observe\n",
    "- Different types: `Box` (continuous), `Discrete` (categorical), `MultiDiscrete`, etc.\n",
    "\n",
    "### 2. **Action Space** (`env.action_space`)\n",
    "- Defines what actions the agent can take\n",
    "- Determines the output of your policy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the observation space\n",
    "print(\"=== OBSERVATION SPACE ===\")\n",
    "print(f\"Type: {type(env.observation_space).__name__}\")\n",
    "print(f\"Shape: {env.observation_space.shape}\")\n",
    "print(f\"High bounds: {env.observation_space.high}\")\n",
    "print(f\"Low bounds: {env.observation_space.low}\")\n",
    "print(f\"Data type: {env.observation_space.dtype}\")\n",
    "\n",
    "print(\"\\n=== ACTION SPACE ===\")\n",
    "print(f\"Type: {type(env.action_space).__name__}\")\n",
    "print(f\"Number of actions: {env.action_space.n}\")\n",
    "print(f\"Actions: 0 = Push cart left, 1 = Push cart right\")\n",
    "\n",
    "# Sample random observations and actions\n",
    "print(\"\\n=== SAMPLING FROM SPACES ===\")\n",
    "print(f\"Random observation sample: {env.observation_space.sample()}\")\n",
    "print(f\"Random action sample: {env.action_space.sample()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83e\udde0 Understanding CartPole Observations\n",
    "\n",
    "The observation is a 4-dimensional vector:\n",
    "1. **Cart Position**: Horizontal position of the cart\n",
    "2. **Cart Velocity**: Speed of the cart\n",
    "3. **Pole Angle**: Angle of the pole (in radians)\n",
    "4. **Pole Angular Velocity**: How fast the pole is rotating\n",
    "\n",
    "**Goal**: Keep the pole balanced (upright) by moving the cart left or right!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83e\uddea Exercise 1: Your Turn to Practice!\n",
    "\n",
    "Now it's your turn! Complete the following code by filling in the blanks. This will help you understand the Gymnasium API.\n",
    "\n",
    "**Your Task**: Complete the `my_first_episode()` function below.\n",
    "\n",
    "```python\n",
    "def my_first_episode():\n",
    "    \"\"\"\n",
    "    Run one episode and collect statistics.\n",
    "    \n",
    "    TODO: Fill in the missing code!\n",
    "    \"\"\"\n",
    "    # 1. Create the CartPole environment with render_mode=\"rgb_array\"\n",
    "    env = gym.make(___________)\n",
    "    \n",
    "    # 2. Reset the environment with seed=42\n",
    "    observation, info = env.___________\n",
    "    \n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # 3. Sample a random action from the action space\n",
    "        action = env.action_space.___________\n",
    "        \n",
    "        # 4. Take the action in the environment\n",
    "        observation, reward, terminated, truncated, info = env.___________(action)\n",
    "        \n",
    "        # 5. Update total reward\n",
    "        total_reward += ___________\n",
    "        steps += 1\n",
    "        \n",
    "        # 6. Check if episode is done (terminated OR truncated)\n",
    "        done = ___________ or ___________\n",
    "    \n",
    "    # 7. Don't forget to close the environment!\n",
    "    env.___________\n",
    "    \n",
    "    return total_reward, steps\n",
    "\n",
    "# Test your function\n",
    "# reward, steps = my_first_episode()\n",
    "# print(f\"Episode reward: {reward}, Steps: {steps}\")\n",
    "```\n",
    "\n",
    "**Hints:**\n",
    "- Line 1: Environment name is \"CartPole-v1\"\n",
    "- Line 2: Method to start a new episode\n",
    "- Line 3: Method to get a random action\n",
    "- Line 4: Method to execute an action\n",
    "- Line 5: What value should we add to total_reward?\n",
    "- Line 6: Episode is done when either condition is True\n",
    "- Line 7: Cleanup method\n",
    "\n",
    "Try it yourself in the cell below! \ud83d\udc47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE - Complete the function!\n",
    "\n",
    "def my_first_episode():\n",
    "    \"\"\"\n",
    "    Run one episode and collect statistics.\n",
    "    \n",
    "    TODO: Fill in the missing code!\n",
    "    \"\"\"\n",
    "    # 1. Create the CartPole environment with render_mode=\"rgb_array\"\n",
    "    env = gym.make(\"___________\", render_mode=\"rgb_array\")  # FILL THIS\n",
    "    \n",
    "    # 2. Reset the environment with seed=42\n",
    "    observation, info = env.___________(seed=42)  # FILL THIS\n",
    "    \n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # 3. Sample a random action from the action space\n",
    "        action = env.action_space.___________()  # FILL THIS\n",
    "        \n",
    "        # 4. Take the action in the environment\n",
    "        observation, reward, terminated, truncated, info = env.___________(action)  # FILL THIS\n",
    "        \n",
    "        # 5. Update total reward\n",
    "        total_reward += ___________  # FILL THIS\n",
    "        steps += 1\n",
    "        \n",
    "        # 6. Check if episode is done (terminated OR truncated)\n",
    "        done = ___________ or ___________  # FILL THIS\n",
    "    \n",
    "    # 7. Don't forget to close the environment!\n",
    "    env.___________()  # FILL THIS\n",
    "    \n",
    "    return total_reward, steps\n",
    "\n",
    "# Uncomment to test your function\n",
    "# reward, steps = my_first_episode()\n",
    "# print(f\"\u2705 Episode reward: {reward}, Steps: {steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \u2705 Solution\n",
    "\n",
    "<details>\n",
    "<summary>Click here to see the solution (try it yourself first!)</summary>\n",
    "\n",
    "```python\n",
    "def my_first_episode():\n",
    "    # 1. Create the CartPole environment\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "    \n",
    "    # 2. Reset the environment\n",
    "    observation, info = env.reset(seed=42)\n",
    "    \n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # 3. Sample a random action\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        # 4. Take the action\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        # 5. Update total reward\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        \n",
    "        # 6. Check if done\n",
    "        done = terminated or truncated\n",
    "    \n",
    "    # 7. Close the environment\n",
    "    env.close()\n",
    "    \n",
    "    return total_reward, steps\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The RL Interaction Loop: `reset()` and `step()`\n",
    "\n",
    "These are the two most important methods in the Gymnasium API:\n",
    "\n",
    "### `env.reset()` - Start a New Episode\n",
    "Returns: `(observation, info)`\n",
    "- **observation**: Initial state of the environment\n",
    "- **info**: Additional diagnostic information (dict)\n",
    "\n",
    "### `env.step(action)` - Take an Action\n",
    "Returns: `(observation, reward, terminated, truncated, info)`\n",
    "- **observation**: New state after taking the action\n",
    "- **reward**: Immediate reward received (float)\n",
    "- **terminated**: Episode ended naturally (e.g., goal reached or failed)\n",
    "- **truncated**: Episode ended due to time limit\n",
    "- **info**: Additional information\n",
    "\n",
    "**Note**: Episode is done when `terminated OR truncated == True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment to start a new episode\n",
    "observation, info = env.reset(seed=SEED)\n",
    "\n",
    "print(\"=== AFTER RESET ===\")\n",
    "print(f\"Initial observation: {observation}\")\n",
    "print(f\"Info: {info}\")\n",
    "print(f\"Observation shape: {observation.shape}\")\n",
    "\n",
    "# Take a random action\n",
    "action = env.action_space.sample()  # Random action (0 or 1)\n",
    "print(f\"\\nTaking action: {action}\")\n",
    "\n",
    "# Execute the action\n",
    "observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "print(\"\\n=== AFTER STEP ===\")\n",
    "print(f\"New observation: {observation}\")\n",
    "print(f\"Reward received: {reward}\")\n",
    "print(f\"Episode terminated?: {terminated}\")\n",
    "print(f\"Episode truncated?: {truncated}\")\n",
    "print(f\"Episode done?: {terminated or truncated}\")\n",
    "print(f\"Info: {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2\ufe0f\u20e3 Visualizing Environments\n",
    "\n",
    "## Rendering a Single Frame\n",
    "\n",
    "Let's visualize what the environment looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset and render the initial state\n",
    "env.reset(seed=SEED)\n",
    "frame = env.render()  # Returns RGB array when render_mode=\"rgb_array\"\n",
    "\n",
    "# Display the frame\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(frame)\n",
    "plt.title(\"CartPole-v1 Initial State\", fontsize=14, fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Frame shape: {frame.shape} (height, width, RGB channels)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Full Episode with Random Actions\n",
    "\n",
    "Let's see how a **random agent** (taking random actions) performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_random_episode(env, seed=None, render=True):\n",
    "    \"\"\"\n",
    "    Run one episode with random actions.\n",
    "    \n",
    "    Args:\n",
    "        env: Gymnasium environment\n",
    "        seed: Random seed for reproducibility\n",
    "        render: Whether to collect frames for visualization\n",
    "    \n",
    "    Returns:\n",
    "        total_reward: Sum of all rewards in the episode\n",
    "        frames: List of RGB arrays (if render=True)\n",
    "    \"\"\"\n",
    "    observation, info = env.reset(seed=seed)\n",
    "    frames = []\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    \n",
    "    # Episode loop: continue until done\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Render and save frame\n",
    "        if render:\n",
    "            frames.append(env.render())\n",
    "        \n",
    "        # Take random action\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        # Execute action in environment\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        # Accumulate reward\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        \n",
    "        # Check if episode is complete\n",
    "        done = terminated or truncated\n",
    "    \n",
    "    print(f\"Episode finished after {steps} steps\")\n",
    "    print(f\"Total reward: {total_reward}\")\n",
    "    \n",
    "    return total_reward, frames\n",
    "\n",
    "# Run one episode\n",
    "reward, frames = run_random_episode(env, seed=SEED)\n",
    "print(f\"\\nCollected {len(frames)} frames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a GIF Visualization\n",
    "\n",
    "Let's create an animated GIF to see the agent in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_frames_as_gif(frames, filename, fps=30):\n",
    "    \"\"\"\n",
    "    Save a list of frames as an animated GIF.\n",
    "    \n",
    "    Args:\n",
    "        frames: List of RGB arrays\n",
    "        filename: Output filename (e.g., 'episode.gif')\n",
    "        fps: Frames per second\n",
    "    \"\"\"\n",
    "    imageio.mimsave(filename, frames, fps=fps)\n",
    "    print(f\"\u2705 Saved GIF to: {filename}\")\n",
    "\n",
    "# Save the random agent episode\n",
    "save_frames_as_gif(frames, '/tmp/random_agent_cartpole.gif')\n",
    "\n",
    "# Display the GIF\n",
    "display(Image(filename='/tmp/random_agent_cartpole.gif'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcca Baseline Performance: Random Agent\n",
    "\n",
    "Let's evaluate the random agent over multiple episodes to establish a baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multiple episodes to get average performance\n",
    "n_episodes = 100\n",
    "random_rewards = []\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    reward, _ = run_random_episode(env, seed=episode, render=False)\n",
    "    random_rewards.append(reward)\n",
    "\n",
    "# Calculate statistics\n",
    "mean_reward = np.mean(random_rewards)\n",
    "std_reward = np.std(random_rewards)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"RANDOM AGENT PERFORMANCE ({n_episodes} episodes)\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Mean reward: {mean_reward:.2f} \u00b1 {std_reward:.2f}\")\n",
    "print(f\"Min reward: {min(random_rewards):.2f}\")\n",
    "print(f\"Max reward: {max(random_rewards):.2f}\")\n",
    "\n",
    "# Visualize distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(random_rewards, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.axvline(mean_reward, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_reward:.2f}')\n",
    "plt.xlabel('Episode Reward', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Random Agent Performance Distribution - CartPole', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3\ufe0f\u20e3 Complete Training Pipeline with PPO\n",
    "\n",
    "Now let's train an intelligent agent using **Proximal Policy Optimization (PPO)**!\n",
    "\n",
    "## What is PPO?\n",
    "\n",
    "**PPO** is a state-of-the-art reinforcement learning algorithm that:\n",
    "- Is relatively easy to tune\n",
    "- Works well across many environments\n",
    "- Balances exploration and exploitation effectively\n",
    "- Is the default choice for many RL practitioners\n",
    "\n",
    "### Key Hyperparameters:\n",
    "- **`policy`**: Network architecture (\"MlpPolicy\" = Multi-Layer Perceptron)\n",
    "- **`learning_rate`**: How fast the agent learns (default: 3e-4)\n",
    "- **`n_steps`**: Steps per environment per update (default: 2048)\n",
    "- **`batch_size`**: Minibatch size (default: 64)\n",
    "- **`n_epochs`**: Number of epochs per update (default: 10)\n",
    "- **`gamma`**: Discount factor for future rewards (default: 0.99)\n",
    "- **`verbose`**: Print training progress (0=none, 1=info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create Training Environment\n",
    "\n",
    "For training, we don't need rendering (it slows things down):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close previous environment\n",
    "env.close()\n",
    "\n",
    "# Create new environment for training (no rendering)\n",
    "train_env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(\"\u2705 Training environment created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize the PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PPO model\n",
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",          # Use Multi-Layer Perceptron policy\n",
    "    env=train_env,                # Training environment\n",
    "    learning_rate=3e-4,           # Learning rate for optimizer\n",
    "    n_steps=2048,                 # Steps per update\n",
    "    batch_size=64,                # Minibatch size\n",
    "    n_epochs=10,                  # Epochs per update\n",
    "    gamma=0.99,                   # Discount factor\n",
    "    verbose=1,                    # Print training info\n",
    "    tensorboard_log=\"/home/rl/workspace/logs\",  # TensorBoard logging directory\n",
    "    seed=SEED                     # For reproducibility\n",
    ")\n",
    "\n",
    "print(\"\\n\u2705 PPO agent initialized!\")\n",
    "print(f\"Policy architecture: {model.policy}\")\n",
    "print(f\"\ud83d\udcca TensorBoard logs will be saved to: /home/rl/workspace/logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train the Agent\n",
    "\n",
    "Now we train the agent for a specified number of timesteps. This is where the magic happens! \ud83c\udfa9\u2728\n",
    "\n",
    "**Note**: Training typically takes a few minutes depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "print(\"\ud83d\ude80 Starting training...\\n\")\n",
    "\n",
    "# For CartPole, 50,000 timesteps is usually enough\n",
    "model.learn(\n",
    "    total_timesteps=50_000,\n",
    "    progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"\\n\u2705 Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcca Monitoring Training with TensorBoard\n",
    "\n",
    "During training, SB3 automatically logs important metrics to TensorBoard. Your logs are saved at: `/home/rl/workspace/logs`\n",
    "\n",
    "**Key Metrics You'll See:**\n",
    "\n",
    "1. **`rollout/ep_rew_mean`**: Average episode reward over time (learning curve)\n",
    "2. **`rollout/ep_len_mean`**: Average episode length\n",
    "3. **`train/entropy_loss`**: Policy entropy (exploration measure)\n",
    "4. **`train/policy_gradient_loss`**: Policy gradient loss\n",
    "5. **`train/value_loss`**: Value function loss\n",
    "6. **`train/approx_kl`**: KL divergence (how much policy changed)\n",
    "7. **`train/clip_fraction`**: Fraction of samples clipped by PPO\n",
    "8. **`train/explained_variance`**: How well value function predicts returns\n",
    "\n",
    "**What to Look For:**\n",
    "- \u2705 **`ep_rew_mean` should increase** - Agent is learning!\n",
    "- \u2705 **Losses should stabilize** - Training is converging\n",
    "- \u26a0\ufe0f **If `ep_rew_mean` plateaus too early** - Try different hyperparameters\n",
    "- \u26a0\ufe0f **If losses explode** - Reduce learning rate\n",
    "\n",
    "Since TensorBoard is already running and monitoring `/home/rl/workspace/logs`, you can view these metrics in real-time! \ud83d\udcc8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Save the Trained Model\n",
    "\n",
    "Always save your trained models so you can reuse them later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_path = \"/tmp/ppo_cartpole\"\n",
    "model.save(model_path)\n",
    "\n",
    "print(f\"\u2705 Model saved to: {model_path}\")\n",
    "\n",
    "# You can load it later with:\n",
    "# loaded_model = PPO.load(model_path, env=train_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the Trained Agent\n",
    "\n",
    "Let's see how well our trained agent performs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the agent\n",
    "mean_reward, std_reward = evaluate_policy(\n",
    "    model, \n",
    "    train_env, \n",
    "    n_eval_episodes=100,\n",
    "    deterministic=True  # Use deterministic actions (no exploration)\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"TRAINED PPO AGENT PERFORMANCE (100 episodes)\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Mean reward: {mean_reward:.2f} \u00b1 {std_reward:.2f}\")\n",
    "print(f\"\\n\ud83c\udfaf Maximum possible reward in CartPole: 500\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Visualize Trained Agent Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trained_episode(model, env, seed=None):\n",
    "    \"\"\"\n",
    "    Run one episode with a trained agent.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained SB3 model\n",
    "        env: Gymnasium environment (with render_mode=\"rgb_array\")\n",
    "        seed: Random seed\n",
    "    \n",
    "    Returns:\n",
    "        total_reward: Episode reward\n",
    "        frames: List of frames\n",
    "    \"\"\"\n",
    "    observation, info = env.reset(seed=seed)\n",
    "    frames = []\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        frames.append(env.render())\n",
    "        \n",
    "        # Use trained model to predict action (deterministic)\n",
    "        action, _ = model.predict(observation, deterministic=True)\n",
    "        \n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        done = terminated or truncated\n",
    "    \n",
    "    print(f\"Episode finished after {steps} steps\")\n",
    "    print(f\"Total reward: {total_reward}\")\n",
    "    \n",
    "    return total_reward, frames\n",
    "\n",
    "# Create environment with rendering\n",
    "eval_env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "# Run trained agent\n",
    "trained_reward, trained_frames = run_trained_episode(model, eval_env, seed=SEED)\n",
    "\n",
    "# Save and display\n",
    "save_frames_as_gif(trained_frames, '/tmp/trained_agent_cartpole.gif')\n",
    "display(Image(filename='/tmp/trained_agent_cartpole.gif'))\n",
    "\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Compare Random vs. Trained Agent\n",
    "\n",
    "Let's visualize the improvement!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare performance\n",
    "comparison_data = {\n",
    "    'Random Agent': np.mean(random_rewards),\n",
    "    'Trained PPO': mean_reward\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(comparison_data.keys(), comparison_data.values(), \n",
    "               color=['lightcoral', 'lightgreen'], alpha=0.8, edgecolor='black', linewidth=2)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.2f}',\n",
    "             ha='center', va='bottom', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.ylabel('Mean Episode Reward', fontsize=12)\n",
    "plt.title('Performance Comparison: Random vs. Trained Agent - CartPole', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.axhline(y=500, color='gold', linestyle='--', linewidth=2, label='Maximum Possible (500)')\n",
    "plt.ylim(0, 550)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate improvement\n",
    "improvement = ((mean_reward - np.mean(random_rewards)) / np.mean(random_rewards)) * 100\n",
    "print(f\"\\n\ud83c\udf89 Improvement: {improvement:.1f}% better than random!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4\ufe0f\u20e3 Second Environment: LunarLander-v3\n",
    "\n",
    "Let's apply the same pipeline to a more challenging environment!\n",
    "\n",
    "## About LunarLander\n",
    "\n",
    "- **Goal**: Land a spacecraft safely on the moon\n",
    "- **Observation Space**: 8 dimensions (position, velocity, angle, etc.)\n",
    "- **Action Space**: 4 discrete actions (do nothing, fire left, fire main, fire right)\n",
    "- **Rewards**: \n",
    "  - +100 for landing safely\n",
    "  - -100 for crashing\n",
    "  - Penalties for fuel usage and distance from landing pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LunarLander environment\n",
    "lunar_env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
    "\n",
    "print(\"=== LUNARLAND ER-V3 ENVIRONMENT ===\")\n",
    "print(f\"Observation space: {lunar_env.observation_space}\")\n",
    "print(f\"  - Shape: {lunar_env.observation_space.shape}\")\n",
    "print(f\"  - 8 continuous values: x, y, vel_x, vel_y, angle, angular_vel, leg1_contact, leg2_contact\")\n",
    "\n",
    "print(f\"\\nAction space: {lunar_env.action_space}\")\n",
    "print(f\"  - 0: Do nothing\")\n",
    "print(f\"  - 1: Fire left engine\")\n",
    "print(f\"  - 2: Fire main engine\")\n",
    "print(f\"  - 3: Fire right engine\")\n",
    "\n",
    "# Visualize initial state\n",
    "lunar_env.reset(seed=SEED)\n",
    "frame = lunar_env.render()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(frame)\n",
    "plt.title(\"LunarLander-v3 Initial State\", fontsize=14, fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Random Agent on LunarLander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate random agent\n",
    "print(\"Testing random agent on LunarLander...\\n\")\n",
    "\n",
    "lunar_random_rewards = []\n",
    "for episode in range(100):\n",
    "    reward, _ = run_random_episode(lunar_env, seed=episode, render=False)\n",
    "    lunar_random_rewards.append(reward)\n",
    "\n",
    "lunar_random_mean = np.mean(lunar_random_rewards)\n",
    "lunar_random_std = np.std(lunar_random_rewards)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"RANDOM AGENT - LUNARLAND ER (100 episodes)\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Mean reward: {lunar_random_mean:.2f} \u00b1 {lunar_random_std:.2f}\")\n",
    "\n",
    "# Show one episode\n",
    "print(\"\\nRecording one random episode...\")\n",
    "_, random_lunar_frames = run_random_episode(lunar_env, seed=SEED, render=True)\n",
    "save_frames_as_gif(random_lunar_frames, '/tmp/random_lunar.gif', fps=20)\n",
    "display(Image(filename='/tmp/random_lunar.gif'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training PPO on LunarLander\n",
    "\n",
    "LunarLander is more complex, so we'll train for more timesteps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close rendering environment\n",
    "lunar_env.close()\n",
    "\n",
    "# Create training environment\n",
    "lunar_train_env = gym.make(\"LunarLander-v3\")\n",
    "\n",
    "# Initialize PPO\n",
    "lunar_model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=lunar_train_env,\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=2048,\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    gamma=0.99,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"/home/rl/workspace/logs\",  # TensorBoard logging\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "print(\"\\n\ud83d\ude80 Training PPO on LunarLander...\")\n",
    "print(\"\u23f0 This will take a few minutes...\")\n",
    "print(\"\ud83d\udcca TensorBoard logs: /home/rl/workspace/logs\\n\")\n",
    "\n",
    "# Train for more timesteps (LunarLander is harder)\n",
    "lunar_model.learn(\n",
    "    total_timesteps=300_000,\n",
    "    progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"\\n\u2705 Training complete!\")\n",
    "\n",
    "# Save model\n",
    "lunar_model.save(\"/tmp/ppo_lunar_lander\")\n",
    "print(\"\u2705 Model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Trained LunarLander Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "lunar_mean_reward, lunar_std_reward = evaluate_policy(\n",
    "    lunar_model,\n",
    "    lunar_train_env,\n",
    "    n_eval_episodes=100,\n",
    "    deterministic=True\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"TRAINED PPO - LUNARLAND ER (100 episodes)\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Mean reward: {lunar_mean_reward:.2f} \u00b1 {lunar_std_reward:.2f}\")\n",
    "print(f\"\\n\ud83c\udfaf A score > 200 is considered solved!\")\n",
    "\n",
    "# Visualize trained agent\n",
    "lunar_eval_env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
    "print(\"\\nRecording trained agent...\")\n",
    "_, trained_lunar_frames = run_trained_episode(lunar_model, lunar_eval_env, seed=SEED)\n",
    "save_frames_as_gif(trained_lunar_frames, '/tmp/trained_lunar.gif', fps=20)\n",
    "display(Image(filename='/tmp/trained_lunar.gif'))\n",
    "\n",
    "lunar_eval_env.close()\n",
    "lunar_train_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LunarLander Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare LunarLander performance\n",
    "lunar_comparison = {\n",
    "    'Random Agent': lunar_random_mean,\n",
    "    'Trained PPO': lunar_mean_reward\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(lunar_comparison.keys(), lunar_comparison.values(),\n",
    "               color=['lightcoral', 'lightgreen'], alpha=0.8, edgecolor='black', linewidth=2)\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.2f}',\n",
    "             ha='center', va='bottom', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.ylabel('Mean Episode Reward', fontsize=12)\n",
    "plt.title('Performance Comparison: Random vs. Trained Agent - LunarLander',\n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.axhline(y=200, color='gold', linestyle='--', linewidth=2, label='Solved Threshold (200)')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate improvement\n",
    "if lunar_random_mean < 0:\n",
    "    improvement = lunar_mean_reward - lunar_random_mean\n",
    "    print(f\"\\n\ud83c\udf89 Improvement: +{improvement:.1f} points from random baseline!\")\n",
    "else:\n",
    "    improvement = ((lunar_mean_reward - lunar_random_mean) / abs(lunar_random_mean)) * 100\n",
    "    print(f\"\\n\ud83c\udf89 Improvement: {improvement:.1f}% better than random!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5\ufe0f\u20e3 Creating Custom Gymnasium Environments\n",
    "\n",
    "Sometimes you need to create your own environment for a specific problem. Let's learn how!\n",
    "\n",
    "## \ud83c\udfae Problem Description: Simple GridWorld\n",
    "\n",
    "We'll create a simple grid world environment where:\n",
    "\n",
    "### Environment Rules:\n",
    "- **Grid Size**: 5x5 grid\n",
    "- **Agent**: Starts at position (0, 0) - top-left corner\n",
    "- **Goal**: Reach position (4, 4) - bottom-right corner\n",
    "- **Actions**: 4 possible actions\n",
    "  - 0: Move UP\n",
    "  - 1: Move DOWN\n",
    "  - 2: Move LEFT\n",
    "  - 3: Move RIGHT\n",
    "- **Rewards**:\n",
    "  - +10 for reaching the goal\n",
    "  - -1 for each step (encourages finding shortest path)\n",
    "  - -5 for hitting walls (trying to move outside grid)\n",
    "- **Episode Termination**: When agent reaches goal or after 100 steps\n",
    "\n",
    "### Visual Representation:\n",
    "```\n",
    "S . . . .    S = Start (0,0)\n",
    ". . . . .    G = Goal (4,4)\n",
    ". . . . .    . = Empty cell\n",
    ". . . . .\n",
    ". . . . G\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfd7\ufe0f Building the Custom Environment\n",
    "\n",
    "Every Gymnasium environment must inherit from `gym.Env` and implement these key methods:\n",
    "\n",
    "### Required Methods:\n",
    "\n",
    "1. **`__init__()`**: Initialize the environment\n",
    "   - Define observation_space and action_space\n",
    "   - Set up any necessary variables\n",
    "\n",
    "2. **`reset()`**: Reset environment to initial state\n",
    "   - Return: `(observation, info)`\n",
    "   - Must handle the `seed` parameter for reproducibility\n",
    "\n",
    "3. **`step(action)`**: Execute one action\n",
    "   - Return: `(observation, reward, terminated, truncated, info)`\n",
    "   - Update environment state\n",
    "   - Calculate reward\n",
    "   - Check if episode is done\n",
    "\n",
    "4. **`render()`** (optional): Visualize the environment\n",
    "   - Return visualization based on render_mode\n",
    "\n",
    "Let's implement each part step by step! \ud83d\udc47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom GridWorld Environment\n",
    "    \n",
    "    Agent navigates a 5x5 grid from start (0,0) to goal (4,4)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Metadata for the environment\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
    "    \n",
    "    def __init__(self, render_mode=None, grid_size=5):\n",
    "        \"\"\"\n",
    "        Initialize the GridWorld environment.\n",
    "        \n",
    "        What we need to define here:\n",
    "        - observation_space: What the agent observes\n",
    "        - action_space: What actions the agent can take\n",
    "        - Internal state variables\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.grid_size = grid_size\n",
    "        self.render_mode = render_mode\n",
    "        \n",
    "        # Define action space: 4 discrete actions (UP, DOWN, LEFT, RIGHT)\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        \n",
    "        # Define observation space: agent's (x, y) position\n",
    "        # Box space with shape (2,) for [x, y] coordinates\n",
    "        # Values range from 0 to grid_size-1\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0,\n",
    "            high=grid_size - 1,\n",
    "            shape=(2,),\n",
    "            dtype=np.int32\n",
    "        )\n",
    "        \n",
    "        # Define start and goal positions\n",
    "        self.start_pos = np.array([0, 0], dtype=np.int32)\n",
    "        self.goal_pos = np.array([grid_size - 1, grid_size - 1], dtype=np.int32)\n",
    "        \n",
    "        # Current agent position (will be set in reset)\n",
    "        self.agent_pos = None\n",
    "        self.num_steps = 0\n",
    "        \n",
    "        print(\"\u2705 GridWorld environment initialized!\")\n",
    "        print(f\"   Grid size: {grid_size}x{grid_size}\")\n",
    "        print(f\"   Start: {self.start_pos}, Goal: {self.goal_pos}\")\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Reset the environment to initial state.\n",
    "        \n",
    "        What we need to do:\n",
    "        - Handle the seed for reproducibility\n",
    "        - Reset agent to start position\n",
    "        - Reset step counter\n",
    "        - Return (observation, info)\n",
    "        \"\"\"\n",
    "        # Seed the random number generator\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # Reset agent to start position\n",
    "        self.agent_pos = self.start_pos.copy()\n",
    "        self.num_steps = 0\n",
    "        \n",
    "        # Get observation (agent's current position)\n",
    "        observation = self.agent_pos.copy()\n",
    "        \n",
    "        # Info dict (can contain any additional information)\n",
    "        info = {\"distance_to_goal\": self._get_distance_to_goal()}\n",
    "        \n",
    "        return observation, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Execute one action and return the result.\n",
    "        \n",
    "        What we need to do:\n",
    "        1. Update environment state based on action\n",
    "        2. Calculate reward\n",
    "        3. Check if episode is terminated or truncated\n",
    "        4. Return (observation, reward, terminated, truncated, info)\n",
    "        \"\"\"\n",
    "        self.num_steps += 1\n",
    "        \n",
    "        # Store old position to check if move was valid\n",
    "        old_pos = self.agent_pos.copy()\n",
    "        \n",
    "        # Execute action: 0=UP, 1=DOWN, 2=LEFT, 3=RIGHT\n",
    "        if action == 0:  # UP\n",
    "            self.agent_pos[1] = max(0, self.agent_pos[1] - 1)\n",
    "        elif action == 1:  # DOWN\n",
    "            self.agent_pos[1] = min(self.grid_size - 1, self.agent_pos[1] + 1)\n",
    "        elif action == 2:  # LEFT\n",
    "            self.agent_pos[0] = max(0, self.agent_pos[0] - 1)\n",
    "        elif action == 3:  # RIGHT\n",
    "            self.agent_pos[0] = min(self.grid_size - 1, self.agent_pos[0] + 1)\n",
    "        \n",
    "        # Calculate reward\n",
    "        reward = -1  # Step penalty (encourages shorter paths)\n",
    "        \n",
    "        # Check if agent hit a wall (position didn't change)\n",
    "        if np.array_equal(old_pos, self.agent_pos):\n",
    "            reward = -5  # Penalty for hitting wall\n",
    "        \n",
    "        # Check if agent reached the goal\n",
    "        terminated = np.array_equal(self.agent_pos, self.goal_pos)\n",
    "        if terminated:\n",
    "            reward = 10  # Big reward for reaching goal!\n",
    "        \n",
    "        # Check if episode should be truncated (time limit)\n",
    "        truncated = self.num_steps >= 100\n",
    "        \n",
    "        # Get observation\n",
    "        observation = self.agent_pos.copy()\n",
    "        \n",
    "        # Additional info\n",
    "        info = {\n",
    "            \"distance_to_goal\": self._get_distance_to_goal(),\n",
    "            \"num_steps\": self.num_steps\n",
    "        }\n",
    "        \n",
    "        return observation, reward, terminated, truncated, info\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Visualize the environment.\n",
    "        \n",
    "        For simplicity, we'll create a text-based visualization.\n",
    "        \"\"\"\n",
    "        if self.render_mode == \"human\" or self.render_mode == \"rgb_array\":\n",
    "            # Create grid visualization\n",
    "            grid = np.zeros((self.grid_size, self.grid_size), dtype=str)\n",
    "            grid[:, :] = '.'\n",
    "            \n",
    "            # Mark start and goal\n",
    "            grid[self.start_pos[1], self.start_pos[0]] = 'S'\n",
    "            grid[self.goal_pos[1], self.goal_pos[0]] = 'G'\n",
    "            \n",
    "            # Mark agent position\n",
    "            if not np.array_equal(self.agent_pos, self.goal_pos):\n",
    "                grid[self.agent_pos[1], self.agent_pos[0]] = 'A'\n",
    "            else:\n",
    "                grid[self.agent_pos[1], self.agent_pos[0]] = 'W'  # Win!\n",
    "            \n",
    "            # Print grid\n",
    "            print(\"\\n\" + \"=\"*20)\n",
    "            for row in grid:\n",
    "                print(' '.join(row))\n",
    "            print(\"=\"*20)\n",
    "            print(f\"Steps: {self.num_steps}\")\n",
    "            print(f\"Position: {self.agent_pos}\")\n",
    "            \n",
    "            # For rgb_array mode, return the grid as a simple representation\n",
    "            if self.render_mode == \"rgb_array\":\n",
    "                return grid\n",
    "    \n",
    "    def _get_distance_to_goal(self):\n",
    "        \"\"\"Helper function: Manhattan distance to goal\"\"\"\n",
    "        return np.sum(np.abs(self.agent_pos - self.goal_pos))\n",
    "\n",
    "print(\"\u2705 GridWorldEnv class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83e\uddea Testing Our Custom Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our custom environment\n",
    "custom_env = GridWorldEnv(render_mode=\"human\")\n",
    "\n",
    "print(\"\\n=== CUSTOM ENVIRONMENT INFO ===\")\n",
    "print(f\"Observation space: {custom_env.observation_space}\")\n",
    "print(f\"Action space: {custom_env.action_space}\")\n",
    "print(f\"Action meanings: 0=UP, 1=DOWN, 2=LEFT, 3=RIGHT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test reset\n",
    "print(\"\\n=== TESTING RESET ===\")\n",
    "obs, info = custom_env.reset(seed=42)\n",
    "print(f\"Initial observation: {obs}\")\n",
    "print(f\"Info: {info}\")\n",
    "custom_env.render()\n",
    "\n",
    "# Take a few manual steps\n",
    "print(\"\\n=== TAKING MANUAL ACTIONS ===\")\n",
    "\n",
    "# Move RIGHT\n",
    "print(\"\\nAction: RIGHT (3)\")\n",
    "obs, reward, terminated, truncated, info = custom_env.step(3)\n",
    "print(f\"Observation: {obs}, Reward: {reward}\")\n",
    "custom_env.render()\n",
    "\n",
    "# Move DOWN\n",
    "print(\"\\nAction: DOWN (1)\")\n",
    "obs, reward, terminated, truncated, info = custom_env.step(1)\n",
    "print(f\"Observation: {obs}, Reward: {reward}\")\n",
    "custom_env.render()\n",
    "\n",
    "# Try to move UP (back towards start)\n",
    "print(\"\\nAction: UP (0)\")\n",
    "obs, reward, terminated, truncated, info = custom_env.step(0)\n",
    "print(f\"Observation: {obs}, Reward: {reward}\")\n",
    "custom_env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83e\udd16 Training an Agent on Our Custom Environment\n",
    "\n",
    "Now let's train a PPO agent on our custom GridWorld!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment for training (no rendering)\n",
    "train_grid_env = GridWorldEnv()\n",
    "\n",
    "# Initialize PPO\n",
    "grid_model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=train_grid_env,\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=2048,\n",
    "    batch_size=64,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"/home/rl/workspace/logs\",\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "print(\"\\n\ud83d\ude80 Training PPO on GridWorld...\\n\")\n",
    "\n",
    "# Train (GridWorld is simple, so we don't need many steps)\n",
    "grid_model.learn(total_timesteps=20_000, progress_bar=True)\n",
    "\n",
    "print(\"\\n\u2705 Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfac Visualizing the Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained agent\n",
    "print(\"\\n=== TRAINED AGENT DEMONSTRATION ===\")\n",
    "\n",
    "test_env = GridWorldEnv(render_mode=\"human\")\n",
    "obs, info = test_env.reset(seed=42)\n",
    "\n",
    "print(\"\\nInitial state:\")\n",
    "test_env.render()\n",
    "\n",
    "total_reward = 0\n",
    "done = False\n",
    "step = 0\n",
    "\n",
    "while not done and step < 20:  # Limit steps for demonstration\n",
    "    # Get action from trained model\n",
    "    action, _ = grid_model.predict(obs, deterministic=True)\n",
    "    \n",
    "    # Take action\n",
    "    obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "    total_reward += reward\n",
    "    done = terminated or truncated\n",
    "    step += 1\n",
    "    \n",
    "    # Render\n",
    "    action_names = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "    print(f\"\\nStep {step} - Action: {action_names[action]}\")\n",
    "    test_env.render()\n",
    "    \n",
    "    if terminated:\n",
    "        print(\"\\n\ud83c\udf89 GOAL REACHED!\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nTotal reward: {total_reward}\")\n",
    "print(f\"Reached goal in {step} steps!\")\n",
    "\n",
    "test_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udca1 Key Takeaways: Custom Environments\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Environment Structure**:\n",
    "   - Inherit from `gym.Env`\n",
    "   - Define `observation_space` and `action_space` in `__init__`\n",
    "   - Implement `reset()` and `step()` methods\n",
    "\n",
    "2. **`__init__()` Method**:\n",
    "   - Define spaces that match your problem\n",
    "   - Initialize environment parameters\n",
    "   - Set up internal state variables\n",
    "\n",
    "3. **`reset()` Method**:\n",
    "   - Must handle `seed` parameter\n",
    "   - Return `(observation, info)`\n",
    "   - Reset all state variables\n",
    "\n",
    "4. **`step()` Method**:\n",
    "   - Update state based on action\n",
    "   - Calculate reward (design rewards carefully!)\n",
    "   - Determine `terminated` and `truncated`\n",
    "   - Return `(observation, reward, terminated, truncated, info)`\n",
    "\n",
    "5. **Reward Design**:\n",
    "   - Positive rewards for desired behavior (reaching goal)\n",
    "   - Negative rewards for undesired behavior (hitting walls)\n",
    "   - Step penalties to encourage efficiency\n",
    "\n",
    "### \ud83d\ude80 Your Turn!\n",
    "\n",
    "Try modifying the GridWorld environment:\n",
    "- Add obstacles that the agent must avoid\n",
    "- Change the reward structure\n",
    "- Make the grid larger\n",
    "- Add multiple goals\n",
    "- Implement diagonal movement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# 6\ufe0f\u20e3 Summary & Key Takeaways\n\n## \ud83c\udf93 What We Learned\n\n### 1. **Gymnasium API**\n- \u2705 `gym.make()` - Create environments with different render modes\n- \u2705 `env.reset()` - Initialize episodes \u2192 returns `(observation, info)`\n- \u2705 `env.step(action)` - Execute actions \u2192 returns `(obs, reward, terminated, truncated, info)`\n- \u2705 `env.observation_space` & `env.action_space` - Understand environment structure\n- \u2705 `env.render()` - Visualize agent behavior\n- \u2705 `env.close()` - Clean up resources\n\n### 2. **Complete Training Pipeline**\n1. Create environment\n2. Initialize RL algorithm (PPO)\n3. Train with `model.learn()`\n4. Save model with `model.save()`\n5. Evaluate with `evaluate_policy()`\n6. Visualize performance\n\n### 3. **Stable-Baselines3 Workflow**\n```python\n# Initialize\nmodel = PPO(\"MlpPolicy\", env, verbose=1)\n\n# Train\nmodel.learn(total_timesteps=50_000)\n\n# Save\nmodel.save(\"model_name\")\n\n# Load\nmodel = PPO.load(\"model_name\", env=env)\n\n# Use\naction, _ = model.predict(obs, deterministic=True)\n```\n\n### 4. **TensorBoard Monitoring**\n- \u2705 Track learning curves (`ep_rew_mean`)\n- \u2705 Monitor training losses (policy, value, entropy)\n- \u2705 Visualize policy updates (KL divergence, clip fraction)\n- \u2705 Real-time training feedback\n\n### 5. **Custom Environments**\n- \u2705 Inherit from `gym.Env`\n- \u2705 Define observation and action spaces\n- \u2705 Implement `__init__`, `reset`, `step` methods\n- \u2705 Design reward functions carefully\n\n### 6. **Key Insights**\n- **Random agents perform poorly** - RL is necessary for complex tasks\n- **Training time varies** - Simpler environments (CartPole) train faster\n- **Visualization helps** - See what the agent is learning\n- **Evaluation is crucial** - Always benchmark against baselines\n- **Reward design matters** - Shapes agent behavior significantly\n\n---\n\n## \ud83d\ude80 Next Steps\n\n### Experiments to Try:\n1. **Hyperparameter tuning**: Adjust learning rate, batch size, etc.\n2. **Different algorithms**: Try DQN, A2C, or SAC\n3. **Custom rewards**: Modify reward functions for different behaviors\n4. **More environments**: Explore Atari, MuJoCo, or custom environments\n5. **Curriculum learning**: Train on progressively harder tasks\n\n### Resources:\n- \ud83d\udcd6 [Gymnasium Documentation](https://gymnasium.farama.org/)\n- \ud83d\udcd6 [Stable-Baselines3 Docs](https://stable-baselines3.readthedocs.io/)\n- \ud83d\udcc4 [PPO Paper](https://arxiv.org/abs/1707.06347)\n- \ud83c\udf93 [Spinning Up in Deep RL](https://spinningup.openai.com/)\n\n---\n\n## \ud83d\udca1 Discussion Questions\n\n1. Why do we need `render_mode=\"rgb_array\"` for creating GIFs?\n2. What's the difference between `terminated` and `truncated`?\n3. Why use `deterministic=True` during evaluation?\n4. How would you modify the code to train on a custom environment?\n5. What happens if you train for too long? (Hint: overfitting)\n\n---\n\n### \ud83c\udf89 Congratulations!\n\nYou now know how to:\n- \u2705 Use the Gymnasium API\n- \u2705 Visualize agent behavior\n- \u2705 Train RL agents with Stable-Baselines3\n- \u2705 Evaluate and compare performance\n- \u2705 Build complete RL pipelines\n\n**Keep learning and experimenting! \ud83d\ude80**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# \ud83d\udcda Appendix: Quick Reference\n",
    "\n",
    "## Gymnasium API Cheat Sheet\n",
    "\n",
    "```python\n",
    "# Create environment\n",
    "env = gym.make(\"EnvName-v0\", render_mode=\"rgb_array\")\n",
    "\n",
    "# Reset (start new episode)\n",
    "obs, info = env.reset(seed=42)\n",
    "\n",
    "# Take action\n",
    "obs, reward, terminated, truncated, info = env.step(action)\n",
    "done = terminated or truncated\n",
    "\n",
    "# Spaces\n",
    "obs_space = env.observation_space\n",
    "act_space = env.action_space\n",
    "sample_obs = obs_space.sample()\n",
    "sample_act = act_space.sample()\n",
    "\n",
    "# Render\n",
    "frame = env.render()  # Returns RGB array\n",
    "\n",
    "# Cleanup\n",
    "env.close()\n",
    "```\n",
    "\n",
    "## SB3 PPO Cheat Sheet\n",
    "\n",
    "```python\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Create model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Train\n",
    "model.learn(total_timesteps=100_000)\n",
    "\n",
    "# Save/Load\n",
    "model.save(\"path/to/model\")\n",
    "model = PPO.load(\"path/to/model\", env=env)\n",
    "\n",
    "# Predict\n",
    "action, _states = model.predict(obs, deterministic=True)\n",
    "\n",
    "# Evaluate\n",
    "mean_reward, std_reward = evaluate_policy(\n",
    "    model, env, n_eval_episodes=100\n",
    ")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}